## Separate user-level notion of record and format-level

### Problems
  1. User want a natural notion like elements of a list of rows of a data frame. These are too small for R to process efficiently. Experiment with splitting data frames show that with airlines data set (30 cols) sweet spot is around 1000 - 10000
  1. Parsers work better reading bigger chunks, more than a single logical record: read.table, getline. There is no reason not to let them work that way even if the user wants to see the records one at a time. We can always split data before calling map.
  1. The only place where small records matter is when writing from a mapper or combiner to a reducer. Then we have to do what it takes for Hadoop to understand records.
  1. Excessive data coertion. If I write `mapreduce(to.dfs(mtcars), map = function(k,v) v))`, independent of vectorization or any other options that `v)` has to be the same as `mtcars)` and have the same columns and data from a subset of rows, as if we did a `split(mtcars), ...)[[i]]` 
 

### Solutions
  1. All parsers read in big chunks and all writers write in big chunks and the size of the chunk needs to be transparent to the user
  1. The data type of the first chunk read is the data type represented by the file. Right now the model is 
  <ol>
  
    <li> read a record
    <li> append
    <li> go back to 1 until done
    <li> manipulate the list thus defined (so called `structured` option)
    
  </ol>
  
    the new model should be
  
  <ol>  
  
    <li> read a record
    <li> if it is a data frame or matrix, rbind. Otherwise c for lists and vectors
    <li> go back to 1 until done
    <li> don't need 4 no more
  
  </ol>
  
    That is we go from a data model where each record represents the element of a list to a model where each records represents a split of data structure.
    
  1. The vectorized mode refers to user-level notions like number of elements in a list or rows in a data frame, not records in the underlying format, be it native csv or whatever
  1. The no-coertion pledge. All data types stay the same through a mapreduce irrespective of vectorized option. `vectorized` only controls the size of a range, not element vs range. In
  
  ```r
  from.dfs(mapreduce(to.dfs(keyvals(k1,v1)), 
                     map = function(k2,v2) ... keyvals(k3,v3), 
                     reduce = function(k4, vv) ... keyvals(k5, v5))) == 
  keyvals(k6, v6)
  ```
  for some valid range `r`, index `i` and some calls of the `map` and `reduce` functions
  
  ```r
  k1[r,] == k2
  v1[r,] == v2
  k3[i,] == k4 
  v3[i,] == vv[[j]] 
  k5 == k6[r,]
  v5 == v6[r,]
  ```
  for data.frames and matrices; for lists and atomic vectors:
  ```r
  k1[r] == k2
  v1[r] == v2
  k3[[i]] == k4
  v3[[i]] == vv[[j]]
  k5 == k6[r]
  v5 == v6[r]
  ```
  
## The new keyval specification
  
  Definition: the number of records (`n.records`) in a collection is the number of rows if defined (`nrow`), otherwise its length (`length`). The number of records in `NULL` is undefined.
  1. Constructor is `keyval(kk,vv)` with two arguments, a collection of keys and one of values. 
  2. It defines `n.records(kk)` key-value pairs. Hence for example `keyval(1:10, 11:20) defines 10 key-value pairs (old `vectorized` option), whereas `keyval(1, 11:20)` defines a single one. To define complex keys one can always nest them one level down. e.g `keyval(list(1:10), 11:20) defines a single key.
  3. keyval(NULL, 11:20) defines an unspecified number of key-value pairs obtained by splitting (by row if that is an option) the second argument. So for a data frame or matrix, that means a few rows per record, for a list or vector a few elements per record. Order is not guaranteed, nor is the exact size of the split.
  
  
## Invariants:
Definition: `rmr.identical(keyval(x,y), keyval(v,w))` returns `TRUE` *iff* `identical(x[P],v)` and `identical(y[P], w) for some permutation `P` (`identical(x[P,],v)` and `identical(y[P,],w)` when rows are defined).
  1. `from dfs` **is the inverse of** `to.dfs`: 
  
  ```r
  rmr.identical(from.dfs(to.dfs(keyval(x,y), format = fmt), format = fmt), keyval(x,y))
  ``` 
  
  for any format as long as that format can represent the data in `x` and `y`. The `native` format can represent anything, hence the latter restriction is moot.
  2. **Identity** `mapreduce` map only:
  
  ```r
  rmr.identical(from.dfs(mapreduce(to.dfs(keyval(x,y)), map = function(k,v) keyval(k,v))))
  ```
  
  3. **Identity** `mapreduce`, full:
  
  ```r
  rmr.identical(
    from.dfs(
      mapreduce(
        to.dfs(keyval(x, y)), 
        map = function(k,v) keyval(k,v), 
        reduce = function(k,vv) keyval(replicate(length(vv), k),vv))))
  ```
  
## The shuffle and reduce question
  1. For performance reasons, when using `keyval` to define multiple keys and values in a map or reduce function, a simplified serialization format can be selected that does not invoke  the R functions `serialize` and `unserialize`. Therefore for this use case only the arguments to `keyval` will be cast to something that can be encoded in this simplified serialization format. User can test the effects of this cast by calling `rmr.simplify`. The goal is to make this function closer to the identity function over time, as the simplified serialization format is enriched to cover more cases. This limitation is already present but it is not clearly exposed to the user. 
  2. There is no `structured` option. With the new definitions, it is unnecessary on the map side and we realized that the right way to do this conversion on the reduce size was completely application dependent. The two use cases that I have in mind, both from real users, are computing column averages and computing the normal equations. In the first case in `reduce = function(k,vv)` Each element of `vv` contained per column sums for subset of rows and the user sought to arrange them in a matrix to perform again column sums. In the second case each element of `vv` contained a matrix and the user wanted to matrix-sum them with `Reduce('+', vv)`. We have structured data in both cases, but in the second `structured = F` is the correct option. This reveals that we were supporting at the library level a specific use case, which is not a good thing. In exchange for zapping this option, rmr will offer some pre-defined aggregators to cover the most important use cases, like stacking data frames or summing matrices.